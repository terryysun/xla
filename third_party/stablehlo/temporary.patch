diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir b/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
@@ -768,7 +768,7 @@
 // CHECK-PRIMITIVE: %[[MAP:.+]] = linalg.map
 // CHECK-PRIMITIVE-SAME: ins(%[[ARG0]], %[[ARG1]]
 // CHECK-PRIMITIVE-SAME: outs(%[[INIT]] : tensor<?xi1>)
-// CHECK-PRIMITIVE-NEXT: (%[[A:.+]]: complex<f32>, %[[B:.+]]: complex<f32>) {
+// CHECK-PRIMITIVE-NEXT: (%[[A:.+]]: complex<f32>, %[[B:.+]]: complex<f32>, %{{.+}}: i1) {
 // CHECK-PRIMITIVE: %[[RE1:.+]] = complex.re %[[A]] : complex<f32>
 // CHECK-PRIMITIVE: %[[RE2:.+]] = complex.re %[[B]] : complex<f32>
 // CHECK-PRIMITIVE: %[[CMP:.+]] = arith.cmpf oeq, %[[RE1]], %[[RE2]] : f32
diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir b/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
@@ -714,7 +714,7 @@
 // CHECK-PRIMITIVE: linalg.map
 // CHECK-PRIMITIVE-SAME: ins(
 // CHECK-PRIMITIVE-SAME: outs(
-// CHECK-PRIMITIVE-NEXT: (%[[LHS_IN:[a-zA-Z0-9]*]]: bf16, %[[RHS_IN:.*]]: bf16) {
+// CHECK-PRIMITIVE-NEXT: (%[[LHS_IN:[a-zA-Z0-9]*]]: bf16, %[[RHS_IN:.*]]: bf16, %[[RESULT_OUT:.*]]: i1) {
 // CHECK-PRIMITIVE-NEXT:   %[[LHS_INT:.*]] = arith.bitcast %[[LHS_IN]] : bf16 to i16
 // CHECK-PRIMITIVE-NEXT:   %[[LHS_CMP:.*]] = arith.cmpi slt, %[[LHS_INT]], %[[C0]] : i16
 // CHECK-PRIMITIVE-NEXT:   %[[LHS_SUB:.*]] = arith.subi %[[C32767]], %[[LHS_INT]] : i16
@@ -937,7 +937,7 @@
 // CHECK-PRIMITIVE-SAME:   ins(%[[LHS]], %[[RHS]] : tensor<2x?xf32>, tensor<2x?xf32>)
 // CHECK-PRIMITIVE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)
 // CHECK-PRIMITIVE-SAME:   {someattr}
-// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {
+// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %[[RESULT_OUT:.*]]: f32) {
 // CHECK-PRIMITIVE:        %[[RES:.*]] = arith.select %[[PRED_ELEM]], %[[LHS_]], %[[RHS_]] : f32
 // CHECK-PRIMITIVE:        linalg.yield %[[RES]]
 
@@ -978,7 +978,7 @@
 // CHECK-PRIMITIVE-SAME:   ins(%[[LHS]], %[[RHS]] : tensor<2x?xf32>, tensor<2x?xf32>)
 // CHECK-PRIMITIVE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)
 // CHECK-PRIMITIVE-SAME:   {someattr}
-// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {
+// CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %[[RESULT_OUT:.*]]: f32) {
 // CHECK-PRIMITIVE:        linalg.yield %[[LHS_]]
 
 // -----
@@ -1416,7 +1416,7 @@
 
 // CHECK-PRIMITIVE: %[[INIT:.*]] = tensor.empty
 // CHECK-PRIMITIVE: %[[RESULT:.*]] = linalg.map ins(%[[LB]], %[[X]], %[[UB]] : tensor<4xf32>, tensor<4xf32>, tensor<4xf32>) outs(%[[INIT]] : tensor<4xf32>)
-// CHECK-PRIMITIVE: (%[[SCALAR_LB:.*]]: f32, %[[SCALAR_X:.*]]: f32, %[[SCALAR_UB:.*]]: f32)
+// CHECK-PRIMITIVE: (%[[SCALAR_LB:.*]]: f32, %[[SCALAR_X:.*]]: f32, %[[SCALAR_UB:.*]]: f32, %[[RESULT_OUT:.*]]: f32)
 // CHECK-PRIMITIVE:   %[[MAX:.*]] = arith.maximumf %[[SCALAR_LB]], %[[SCALAR_X]] : f32
 // CHECK-PRIMITIVE:   %[[MIN:.*]] = arith.minimumf %[[MAX]], %[[SCALAR_UB]] : f32
 // CHECK-PRIMITIVE:   linalg.yield %[[MIN]]
@@ -1478,7 +1478,7 @@
 // CHECK-PRIMITIVE-DAG: %[[SCALAR_LB:.*]] = tensor.extract %[[LB]]
 // CHECK-PRIMITIVE-DAG: %[[SCALAR_UB:.*]] = tensor.extract %[[UB]]
 // CHECK-PRIMITIVE: %[[RESULT:.*]] = linalg.map ins(%[[X]] : tensor<?xf32>) outs(%[[INIT]] : tensor<?xf32>)
-// CHECK-PRIMITIVE: (%[[SCALAR_X:.*]]: f32)
+// CHECK-PRIMITIVE: (%[[SCALAR_X:.*]]: f32, %[[RESULT_OUT:.*]]: f32)
 // CHECK-PRIMITIVE:   %[[MAX:.*]] = arith.maximumf %[[SCALAR_LB]], %[[SCALAR_X]] : f32
 // CHECK-PRIMITIVE:   %[[MIN:.*]] = arith.minimumf %[[MAX]], %[[SCALAR_UB]] : f32
 // CHECK-PRIMITIVE:   linalg.yield %[[MIN]]
@@ -1554,7 +1554,7 @@
   // CHECK:   linalg.yield %[[V_NOT]] : i32
   // CHECK-PRIMITIVE: %[[CST_N1:.+]] = arith.constant -1 : i32
   // CHECK-PRIMITIVE: linalg.map
-  // CHECK-PRIMITIVE:   (%[[IN:.+]]: i32)
+  // CHECK-PRIMITIVE:   (%[[IN:.+]]: i32, %[[RESULT_OUT:.+]]: i32)
   // CHECK-PRIMITIVE:   %[[V_NOT:.+]] = arith.xori %[[IN]], %[[CST_N1]] : i32
   // CHECK-PRIMITIVE:   linalg.yield %[[V_NOT]] : i32
   %0 = "stablehlo.not"(%arg) : (tensor<2x2xi32>) -> tensor<2x2xi32>
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
@@ -1748,6 +1748,12 @@
 
     rewriter.applySignatureConversion(&region.front(), signatureConverter,
                                       getTypeConverter());
+    auto& blocks = linalgOp.getMapper().getBlocks();
+    if (blocks.empty()) {
+      return rewriter.notifyMatchFailure(op, "expected at least one block");
+    }
+    blocks.front().addArgument(resultType.getElementType(), loc);
+
     auto result = rewriter.createOrFold<tensor::CastOp>(loc, resultType,
                                                         linalgOp.getResults());
     rewriter.replaceOp(op, result);
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
@@ -33,6 +33,7 @@
 
 template <typename OpTy>
 struct ScalarHloToFuncPatterns final : OpConversionPattern<OpTy> {
+  // NOLINTNEXTLINE(clang-diagnostic-shadow-field)
   ScalarHloToFuncPatterns(TypeConverter& typeConverter, MLIRContext* context,
                           PatternBenefit benefit = 1)
       : OpConversionPattern<OpTy>(typeConverter, context, benefit) {}
@@ -51,6 +52,7 @@
 template <typename OpTy>
 struct ScalarHloToArithmeticPattern final : OpConversionPattern<OpTy> {
   ScalarHloToArithmeticPattern(
+      // NOLINTNEXTLINE(clang-diagnostic-shadow-field)
       TypeConverter& typeConverter, MLIRContext* context,
       llvm::function_ref<bool(Operation*)> filterFn = nullptr,
       PatternBenefit benefit = 1)
diff --ruN a/stablehlo/stablehlo/dialect/Base.td b/stablehlo/stablehlo/dialect/Base.td
--- stablehlo/stablehlo/dialect/Base.td
+++ stablehlo/stablehlo/dialect/Base.td
@@ -152,7 +152,7 @@
     AnyTypeOf<[HLO_PerAxisQuantizedSignedInt, HLO_PerAxisQuantizedUnsignedInt], "per-axis integer quantized">;
 
 // Token type.
-def HLO_Token : Type<CPred<"::llvm::isa<::mlir::stablehlo::TokenType>($_self)">, "token">;
+def HLO_Token : Type<CPred<"::llvm::isa<TokenType>($_self)">, "token">;
 
 // Any integer tensor types
 def HLO_IntTensor : RankedTensorOf<[HLO_Int]>;
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -3164,6 +3164,7 @@
 using mlir::hlo::printVariadicOperandWithAttribute;
 using mlir::hlo::printVariadicSameOperandsAndResultType;
 
+using mlir::stablehlo::TokenType;
 #define GET_OP_CLASSES
 #include "stablehlo/dialect/StablehloOps.cpp.inc"
 
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
@@ -20,7 +20,7 @@
 #include <utility>
 #include <vector>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "llvm/ADT/DenseMap.h"
 #include "mlir/IR/BuiltinTypeInterfaces.h"
 #include "mlir/IR/BuiltinTypes.h"
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
@@ -15,7 +15,7 @@
 
 #include <string>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "llvm/Support/raw_ostream.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
 #include "mlir/IR/BuiltinOps.h"
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
@@ -29,9 +29,35 @@
 #include "stablehlo/dialect/TypeInference.h"
 #include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
 #include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
+#include "third_party/llvm/llvm-project/mlir/include/mlir/IR/Attributes.h"
 
 namespace mlir {
 namespace stablehlo {
+
+///////////////
+// Dialect Helpers
+///////////////
+
+MlirOp AttachFrontendAttribute(MlirBuilder& builder, MlirOp op, StringRef name,
+                               Attribute value) {
+  constexpr char kFrontendAttrName[] = "mhlo.frontend_attributes";
+  Operation* mlirOp = unwrap(op).getDefiningOp();
+  SmallVector<NamedAttribute> attrs;
+  DictionaryAttr frontendAttr =
+      mlirOp->getAttrOfType<DictionaryAttr>(kFrontendAttrName);
+  if (frontendAttr) {
+    for (NamedAttribute attr : frontendAttr.getValue()) {
+      // Populate all non-conflicting names.
+      if (attr.getName() != name) {
+        attrs.push_back(attr);
+      }
+    }
+  }
+  attrs.emplace_back(name, value);
+  mlirOp->setAttr(kFrontendAttrName,
+                  DictionaryAttr::get(&builder.getContext(), attrs));
+  return op;
+}
 
 /////////////////
 // MANUAL APIs
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
@@ -27,9 +27,22 @@
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
 #include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
+#include "third_party/llvm/llvm-project/mlir/include/mlir/IR/Attributes.h"
 
 namespace mlir {
 namespace stablehlo {
+
+///////////////
+// Dialect Helpers
+///////////////
+
+// Appends or overwrites an entry in the `mhlo.frontend_attributes` attribute
+//
+// of the given op.
+// Ex:
+//   stablehlo.abs %0 { mhlo.frontend_attributes = { "foo" = 123 } }
+MlirOp AttachFrontendAttribute(MlirBuilder& builder, MlirOp op, StringRef name,
+                               Attribute value);
 
 /////////////////
 // MANUAL APIs
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
@@ -17,7 +17,7 @@
 #include <cstdint>
 #include <string>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "mlir/IR/BuiltinAttributes.h"
 #include "mlir/IR/BuiltinOps.h"
 #include "mlir/IR/DialectRegistry.h"
@@ -1592,5 +1592,57 @@
   EXPECT_EQ(expected, debugString(*module));
 }
 
+TEST(MlirBuilderTest, FrontendAttributesAppend) {
+  std::string expected = R"mlir(module {
+  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {
+    %0 = stablehlo.exponential %arg0 {mhlo.frontend_attributes = {bar = "hello", foo = 123 : i32}} : tensor<2xf32>
+    return %0 : tensor<2xf32>
+  }
+})mlir";
+
+  StablehloModuleBuilder mb;
+  {
+    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
+    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
+    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);
+    auto arg0 = func::Argument(fb, type);
+    auto exp = Exp(arg0);
+    stablehlo::AttachFrontendAttribute(
+        fb, exp, "foo", fb.getOpBuilder().getI32IntegerAttr(123));
+    stablehlo::AttachFrontendAttribute(
+        fb, exp, "bar", fb.getOpBuilder().getStringAttr("hello"));
+    func::Return(fb, {exp});
+  }
+
+  OwningOpRef<ModuleOp> module = mb->build();
+  EXPECT_EQ(expected, debugString(*module));
+}
+
+TEST(MlirBuilderTest, FrontendAttributesOverwrite) {
+  std::string expected = R"mlir(module {
+  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {
+    %0 = stablehlo.exponential %arg0 {mhlo.frontend_attributes = {foo = 456 : i32}} : tensor<2xf32>
+    return %0 : tensor<2xf32>
+  }
+})mlir";
+
+  StablehloModuleBuilder mb;
+  {
+    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
+    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
+    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);
+    auto arg0 = func::Argument(fb, type);
+    auto exp = Exp(arg0);
+    stablehlo::AttachFrontendAttribute(
+        fb, exp, "foo", fb.getOpBuilder().getI32IntegerAttr(123));
+    stablehlo::AttachFrontendAttribute(
+        fb, exp, "foo", fb.getOpBuilder().getI32IntegerAttr(456));
+    func::Return(fb, {exp});
+  }
+
+  OwningOpRef<ModuleOp> module = mb->build();
+  EXPECT_EQ(expected, debugString(*module));
+}
+
 }  // namespace stablehlo
 }  // namespace mlir
diff --ruN a/stablehlo/stablehlo/reference/InterpreterOps.cpp b/stablehlo/stablehlo/reference/InterpreterOps.cpp
--- stablehlo/stablehlo/reference/InterpreterOps.cpp
+++ stablehlo/stablehlo/reference/InterpreterOps.cpp
@@ -46,6 +46,7 @@
 #include "stablehlo/reference/ProcessGrid.h"
 #include "stablehlo/reference/Value.h"
 
+using mlir::stablehlo::TokenType;
 #define GET_OP_CLASSES
 #include "stablehlo/reference/InterpreterOps.cpp.inc"
 
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
@@ -22,21 +22,24 @@
 // CHECK-LABEL: func.func @broadcast_in_dim_fold_splat
 // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x3xi32>)
 func.func @broadcast_in_dim_fold_splat(%arg0: tensor<3x3xi32>)
-  -> (tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>) {
+  -> (tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>) {
   %c0 = stablehlo.constant dense<5> : tensor<i32>
   %c1 = stablehlo.constant dense<3.0> : tensor<f32>
-  %c2 = stablehlo.constant dense<1> : tensor<1x3xi32>
+  %c2 = stablehlo.constant dense<(1.0,2.0)> : tensor<complex<f32>>
+  %c3 = stablehlo.constant dense<1> : tensor<1x3xi32>
 
   %0 = stablehlo.broadcast_in_dim %c0, dims = [] : (tensor<i32>) -> tensor<6xi32>
   %1 = stablehlo.broadcast_in_dim %c1, dims = [] : (tensor<f32>) -> tensor<3xf32>
-  %2 = stablehlo.broadcast_in_dim %c2, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>
+  %2 = stablehlo.broadcast_in_dim %c2, dims = [] : (tensor<complex<f32>>) -> tensor<5xcomplex<f32>>
+  %3 = stablehlo.broadcast_in_dim %c3, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>
 
   // CHECK-DAG:  [[R0:%.+]] = stablehlo.constant dense<5> : tensor<6xi32>
   // CHECK-DAG:  [[R1:%.+]] = stablehlo.constant dense<3.000000e+00> : tensor<3xf32>
-  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>
-
-  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]]
-  return %0, %1, %2 : tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>
+  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<(1.0{{.*}},2.0{{.*}})> : tensor<5xcomplex<f32>>
+  // CHECK-DAG:  [[R3:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>
+
+  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]], [[R3]]
+  return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
@@ -134,6 +134,35 @@
   return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>
 }
 
+// CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape
+// CHECK-SAME:   ([[ARG0:%[^ ]+]]: tensor<3x4xi32>)
+func.func @broadcast_in_dim_prefer_nested_reshape(%arg0: tensor<3x4xi32>) -> (tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>) {
+  // When `broadcast_in_dim(broadcast_in_dim(x))` could be optimized into either
+  // `broadcast_in_dim(reshape(x))` or `broadcast_in_dim(x)`, we want to select
+  // the former pattern.
+  //
+  // (We accomplish this by blocking the merge-composition pattern if the inner
+  // op can be replaced with a `reshape`. Simply adding benefit to the
+  // replace-with-reshape pattern isn't sufficient here because the outermost
+  // op, which only matches the merge-composition pattern, is traversed first.)
+
+  // CHECK-DAG: [[INNER_RESHAPE:%[^ ]+]] = stablehlo.reshape [[ARG0]] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>
+  // CHECK-DAG: [[BROADCAST_OF_RESHAPE:%[^ ]+]] = stablehlo.broadcast_in_dim [[INNER_RESHAPE]], dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>
+  %0 = stablehlo.broadcast_in_dim %arg0, dims = [0, 2] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>
+  %1 = stablehlo.broadcast_in_dim %0, dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>
+
+  // When the inner op doesn't qualify for replacement with a `reshape` op,
+  // however (particularly when it meets some conditions but not others), ensure
+  // that we allow the merge-composition pattern to match.
+
+  // CHECK-DAG: [[MERGED_BROADCAST:%[^ ]+]] = stablehlo.broadcast_in_dim [[ARG0]], dims = [3, 2] : (tensor<3x4xi32>) -> tensor<2x3x4x3xi32>
+  %2 = stablehlo.broadcast_in_dim %arg0, dims = [2, 1] : (tensor<3x4xi32>) -> tensor<1x4x3xi32>
+  %3 = stablehlo.broadcast_in_dim %2, dims = [0, 2, 3] : (tensor<1x4x3xi32>) -> tensor<2x3x4x3xi32>
+
+  // CHECK-DAG: return [[BROADCAST_OF_RESHAPE]], [[MERGED_BROADCAST]]
+  return %1, %3 : tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>
+}
+
 // CHECK-LABEL: func.func @broadcast_in_dim_not_identity_broadcasts
 func.func @broadcast_in_dim_not_identity_broadcasts(%arg0: tensor<1x2xf32>) -> tensor<2x2xf32> {
   // CHECK: stablehlo.broadcast_in_dim
@@ -208,6 +237,18 @@
   // CHECK-NEXT: return [[C1]], [[C0]], [[C1]], [[C0]], [[R0]], [[R1]], [[R2]], [[R3]]
   return %0, %1, %2, %3, %4, %5, %6, %7 :
          tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>
+}
+
+// CHECK-LABEL: func.func @compare_op_bool_simplify
+// CHECK-SAME:   ([[ARG0:%.+]]: tensor<i1>)
+func.func @compare_op_bool_simplify(%arg0: tensor<i1>) -> (tensor<i1>, tensor<i1>) {
+  %false = stablehlo.constant dense<false> : tensor<i1>
+  %true = stablehlo.constant dense<true> : tensor<i1>
+  // CHECK-NOT: stablehlo.compare
+  %0 = stablehlo.compare NE, %arg0, %false, UNSIGNED : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  %1 = stablehlo.compare EQ, %arg0, %true, UNSIGNED : (tensor<i1>, tensor<i1>) -> tensor<i1>
+  // CHECK: return [[ARG0]], [[ARG0]]
+  func.return %0, %1 : tensor<i1>, tensor<i1>
 }
 
 // -----
@@ -1021,6 +1062,18 @@
   // CHECK-NOT: stablehlo.pad
   %1 = stablehlo.pad %arg0, %0, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<256x1024xbf16>, tensor<bf16>) -> tensor<256x1024xbf16>
   return %1 : tensor<256x1024xbf16>
+}
+
+// We don't want to delete `pad` ops that move a tensor's values around without
+// affecting its dimensions.
+//
+// CHECK-LABEL: @pad_rotate_tensor_no_dim_change
+func.func @pad_rotate_tensor_no_dim_change(%arg0: tensor<50x50xf32>) -> tensor<50x50xf32> {
+  // CHECK: %[[RES:.+]] = stablehlo.pad
+  // CHECK: return %[[RES]]
+  %cst = stablehlo.constant dense<0.0> : tensor<f32>
+  %0 = stablehlo.pad %arg0, %cst, low = [0, -1], high = [0, 1], interior = [0, 0] : (tensor<50x50xf32>, tensor<f32>) -> tensor<50x50xf32>
+  return %0 : tensor<50x50xf32>
 }
 
 // -----
@@ -1810,6 +1863,15 @@
   return %0 : tensor<2x4x1x5xf32>
 }
 
+// CHECK-LABEL: @transpose_of_transpose
+func.func @transpose_of_transpose(%arg0 : tensor<1x2x3x4xf32>) -> tensor<1x2x3x4xf32> {
+  %0 = stablehlo.transpose %arg0, dims = [3,2,1,0] : (tensor<1x2x3x4xf32>) -> tensor<4x3x2x1xf32>
+  %1 = stablehlo.transpose %0, dims = [3,2,1,0] : (tensor<4x3x2x1xf32>) -> tensor<1x2x3x4xf32>
+  // CHECK-NOT: stablehlo.transpose
+  // CHECK: return %arg0
+  return %1 : tensor<1x2x3x4xf32>
+}
+
 // -----
 
 ////////
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
@@ -752,7 +752,7 @@
     %2 = call @refine_call_callee(%arg0_different_i32, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %2 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
@@ -770,7 +770,7 @@
     %2 = call @refine_call_callee(%arg0_different, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %2 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
@@ -789,7 +789,7 @@
     %4 = call @refine_call_callee(%arg0_new, %3) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %4 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -461,7 +461,7 @@
   LogicalResult emitDifferentRefinementContextError(func::FuncOp func,
                                                     RefinementKey key,
                                                     RefinementKey prevKey) {
-    return func.emitOpError() << "refined with invompatible refinement keys:"
+    return func.emitOpError() << "refined with incompatible refinement keys:"
                               << "\n  curr=" << key.toString()
                               << "\n  prev=" << prevKey.toString();
   }
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
@@ -530,10 +530,15 @@
   using FoldOpRewritePattern<OpType>::matchAndRewrite;
   using FoldOpRewritePattern<OpType>::options;
 
+  // TODO: Generalize all relevant folder patterns to support complex data
+  // types, then hard-code `allowComplex` to `true`.
   LogicalResult validateShapeFoldDtype(PatternRewriter& rewriter, OpType op,
-                                       ShapedType resultType) const {
+                                       ShapedType resultType,
+                                       bool allowComplex = false) const {
     if (resultType.getElementType().isInteger()) return success();
-    if (options.optimizeFloat && isa<FloatType>(resultType.getElementType()))
+    if (options.optimizeFloat &&
+        (allowComplex ? isa<FloatType, ComplexType>(resultType.getElementType())
+                      : isa<FloatType>(resultType.getElementType())))
       return success();
     return rewriter.notifyMatchFailure(op, "skipping fold of shape op dtype");
   }
@@ -605,7 +610,8 @@
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
-        failed(validateShapeFoldDtype(rewriter, op, resultType)))
+        failed(validateShapeFoldDtype(rewriter, op, resultType,
+                                      /*allowComplex=*/true)))
       return failure();
 
     SplatElementsAttr cstAttr;
@@ -825,7 +831,8 @@
     RankedTensorType resultType = op.getType();
 
     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
-        failed(validateShapeFoldDtype(rewriter, op, resultType)))
+        failed(validateShapeFoldDtype(rewriter, op, resultType)) ||
+        failed(validateElementCountForFold(rewriter, op, resultType)))
       return failure();
 
     auto operandElemType = getElementTypeOrSelf(operand.getType());
@@ -1104,7 +1111,7 @@
         failed(validateShapeFoldDtype(rewriter, op, resultType)))
       return failure();
 
-    DenseIntOrFPElementsAttr attr;
+    DenseElementsAttr attr;
     if (!matchPattern(op.getOperand(), m_Constant(&attr)))
       return rewriter.notifyMatchFailure(op, "expected constant operand");
     rewriter.replaceOpWithNewOp<ConstantOp>(op, attr.reshape(resultType));
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
@@ -1309,10 +1309,20 @@
 // TransposeOp
 /////////////////////////////////
 
+DenseI64ArrayAttr getMergedTransposePermutation(OpBuilder& b,
+                                                ArrayRef<int64_t> childPerm,
+                                                ArrayRef<int64_t> parentPerm) {
+  SmallVector<int64_t> mergedPerm;
+  mergedPerm.reserve(parentPerm.size());
+  for (int64_t parentIdx : parentPerm) {
+    mergedPerm.push_back(childPerm[parentIdx]);
+  }
+  return b.getDenseI64ArrayAttr(mergedPerm);
+}
+
 // Pattern: transpose(X, [no_mem_layout_change...]) -> reshape(X)
 struct TransposeIsReshape final : SimplifyOpRewritePattern<TransposeOp> {
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
-
   LogicalResult matchAndRewrite(TransposeOp op,
                                 PatternRewriter& rewriter) const override {
     auto input = op.getOperand();
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
@@ -43,6 +43,14 @@
     CPred<"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()">,
     "same number of elements">;
 
+def BroadcastNotReducibleToReshape : Constraint<
+    CPred<"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && "
+          "!("
+            "llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && "
+            "llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()"
+          ")">,
+    "is a broadcast_in_dim op that cannot be simplified to a reshape op">;
+
 def OperandsEqual : Constraint<CPred<"$0 == $1">, "operands are equal">;
 
 def RankEqual : Constraint<
@@ -61,6 +69,10 @@
 def AnyZero : AttrConstraint<
     CPred<"::mlir::matchPattern($_self, m_AnyAttrOf(m_Zero(), m_AnyZeroFloat()))">,
     "is int or float zero">;
+
+def ZeroArrayI64 : AttrConstraint<
+    CPred<"::llvm::all_of(::llvm::cast<DenseI64ArrayAttr>($_self).asArrayRef(), [](int64_t val) { return val == 0; })">,
+    "is an array of zeros">;
 
 def DenseIntElementsAttr : AttrConstraint<
     CPred<"llvm::isa<DenseIntElementsAttr>($_self)">,
@@ -120,6 +132,8 @@
 
 def MergeBroadcastDims : NativeCodeCall<"getMergedBroadcastDimensions($_builder, $0, $1)">;
 
+def MergePermutations : NativeCodeCall<"getMergedTransposePermutation($_builder, $0, $1)">;
+
 def StableHLO_ConvertOpWithShape : NativeCodeCall<
     "$_builder.create<stablehlo::ConvertOp>($_loc, $0.getType(), $1)">;
 
@@ -178,18 +192,23 @@
 
 // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])
 //       -> broadcast_in_dim(X, merge(dimsA, dimsB))
+//          [if the nested broadcast can't be simplified to a reshape]
 def BroadcastInDimOp_MergeComposition
-  : Pat<(StableHLO_BroadcastInDimOp
-            (StableHLO_BroadcastInDimOp $operand, $dims_parent), $dims),
+  : Pat<(StableHLO_BroadcastInDimOp:$outer_op
+            (StableHLO_BroadcastInDimOp:$inner_op $operand, $inner_dims),
+            $outer_dims),
         (StableHLO_BroadcastInDimOp
-            $operand, (MergeBroadcastDims $dims, $dims_parent))>;
+            $operand, (MergeBroadcastDims $outer_dims, $inner_dims)),
+        [(BroadcastNotReducibleToReshape $inner_op, $operand)]>;
 
 // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])
 //          [if same numel]
 def BroadcastInDimOp_ReplaceWithReshape
   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, SortedDims:$dims),
         (StableHLO_ReshapeOpWithShape $op, $operand),
-        [(NumberOfElementsEqual $op, $operand)]>;
+        [(NumberOfElementsEqual $op, $operand)],
+        [],
+        (addBenefit 1)>;
 
 // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])
 //          [if same numel & rank]
@@ -197,6 +216,36 @@
   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, $dims),
         (StableHLO_TransposeOp $operand, (InvertBroadcastDims $dims)),
         [(NumberOfElementsEqual $op, $operand), (RankEqual $op, $operand)]>;
+
+////////
+// CompareOp
+
+// The canonical form has the constant operand as the RHS.
+class StableHLO_ComparisonDirectionValue<string enumStr> :
+  ConstantAttr<StableHLO_ComparisonDirectionAttr, "::mlir::stablehlo::ComparisonDirection::" # enumStr>;
+
+// Pattern: compare(NE, X, False) : i1 -> X
+def CompareOp_NeBooleanFalse
+  : Pat<(StableHLO_CompareOp
+            $lhs,
+            (StableHLO_ConstantOp:$cst IntZero:$value),
+            StableHLO_ComparisonDirectionValue<"NE">,
+            $type),
+        (replaceWithValue $lhs),
+        [(HLO_PredTensor $cst)]>;
+
+// Pattern: compare(EQ, X, True) : i1 -> X
+def CompareOp_EqBooleanTrue
+  : Pat<(StableHLO_CompareOp
+            $lhs,
+            (StableHLO_ConstantOp:$cst IntOne:$value),
+            StableHLO_ComparisonDirectionValue<"EQ">,
+            $type),
+        (replaceWithValue $lhs),
+        [(HLO_PredTensor $cst)]>;
+
+// TODO: compare(EQ, X, False) : i1 -> not(X)
+// TODO: compare(NE, X, True) : i1 -> not(X)
 
 ////////
 // ConvertOp
@@ -424,9 +473,9 @@
   : Pat<(StableHLO_PadOp:$pad
             $operand,
             $padding_value,
-            $edge_padding_low,
-            $edge_padding_high,
-            $interior_padding),
+            ZeroArrayI64:$edge_padding_low,
+            ZeroArrayI64:$edge_padding_high,
+            ZeroArrayI64:$interior_padding),
         (replaceWithValue $operand),
         [(TypesEqual $pad, $operand)]>;
 
@@ -539,6 +588,12 @@
   : Pat<(StableHLO_TransposeOp $lhs, IotaDims:$dims),
         (replaceWithValue $lhs)>;
 
+// Pattern: transpose(transpose(X)) -> transpose(X)
+def TransposeOp_TransposeOfTranspose
+  : Pat<(StableHLO_TransposeOp
+          (StableHLO_TransposeOp $child, $child_dims), $dims),
+        (StableHLO_TransposeOp $child, (MergePermutations $child_dims, $dims))>;
+
 ////////
 // GetTupleElementOp
 

